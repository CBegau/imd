<H1>Correlation functions (by Martin Hohl)</H1>

Computation of the van-Hove correlation, method for computation may be
found in the diploma thesis of 
Martin Hohl, p. 47, equation 4.11-4.13, especially 4.12 for seld correlations
equation 4.13 is currently (20.5.98) not implemented, as well as
equation 4.20, a reference implementation may be found in the program
VVMD in the source file vhcf.c.
<P><H2><A NAME="SECTION03311000000000000000">
Modules:</A>
</H2>
<P><TT>init_correl(int ncorr_rmax, int ncorr_tmax)</TT>
<P>
Allocation and initialization of the histogram arrays for the correlation
Initialization of parameters for normalization and scaling.
<P><TT>void correlate(int step, int ref_step)</TT>
<P>
This routine initializes the reference positions if step=refstep, in
each other case the translation rii(tau) will be computed and inserted
in the histogram. Then the mean square displacement is
calculated. Caution: For the computation of the mean square
displacement periodic boundary conditions are applied. If during a
simulation a particle moves a distance longer than the half of the box
dimension, the mean square displacement and the diffusion constant
will be too small in this case. In solids under normal conditions this
may happen only after very long simulation times or at very high
temperatures. In materials with large free volumes this phenomenon
could also appear.
<P><TT>void write_add_corr(int it, int steps)</TT>
<P>
adds new data sets to the correlation file. The number of particle
type is appended to the filename given in the parameter file.
<P><B>Output format:</B>
<P>
The output file will be written in the following way, depending on the
parameter corrrel_omode:
<P>
For correl_omode = 1...3 (gnuplot-Output)
<P>
t  r  G(r,t)
<P>
In case correl_omode = 1 or correl_omode = 2 for each t all
histogram entries G(r,t) are written with ascending r into the file, 
until the last G(r,t) different from zero for this t is reached.
Afterwards at least one addition zero is written to possibly outwit
gridding algorithms which are less robust (set dgrid3d in gnuplot).
Then one or two empty lines (thus correl_omode also fixes the number
of empty rows) are written. With this method relatively compact files
can be written.
<P>
If correl_omode = 3 all values which are zero are written to the
output file. This increases the size of the file tremendously. No
empty rows are written out.
<P>
If correl_omode = 4 a strongly shortened version of the file is written:
<P>
First two header lines are written indicating the step width and the
range of r and t. Then follows for each t the number of r values G(r,t)
and finally the values themselves but only until the last value of
G(r,t) not equal to zero is reached. Finally two zeros are written to
possibly overwit less robust gridding algorithms (set dgrid3d in
gnuplot). This variant of the file can not directly be manipulated in
gnuplot, but must be processed further with a suitable AWK script.
<P><H2><A NAME="SECTION03312000000000000000">
Parallelization</A>
</H2>
<P><B>Self correlation</B>
<P>
The data structture cell is extended by an item for the reference
position at t=t0. This data structure is send by MPI to the
corresponding CPU whenever the particle changes into another CPU.
The communication overhead is thus relatively small, since this case
occures relatively rarely in the solid state.
For the computation of the local correlation function within a CPU no
cummunication via MPI is needed. Only at the end [of the simulation]
the local values have to be added up to the global value over all CPUs
and normalized so that G(r=0,t=0)=1.
<P><B>Distinct correlation</B>
<P>
Since this deals with the calculation of pair correlations each CPU
has to communicated with each other to compute the distances between
the pairs of particles. It is desirable to develop an algorithm where
the communication overhead scales linearly with the number of particles.
<P>
With onventional programming the expense grows with the square of the
particle number. Therefore we must come up with a special scheme:
<P>
<DIV ALIGN="CENTER"><IMG WIDTH="515" HEIGHT="141" ALIGN="BOTTOM" BORDER="0"
 SRC="img32.gif"
 ALT="\includegraphics [width=3cm,angle=-90]{volve.ps.gz}
">
<BR>
</DIV>
<P>
The torus sturcture of the cells ( and of the particle positions therefore)
must be mapped onto a ring suitably. In the two dimensional case this
can be realized as shown above. The data are sent from one CPU to the
neighbouring as indicated by the arrows. We restrict ourselves to non
diagonal paths since the diagonal transport requires two steps on the
parallel computers available to us. For n CPUs it should be achieved
after n steps that each set of values has passed through each CPU ones
and has come back to the CPU where it originated. This algorithm is
linear with respect to the particle number if we assumed that the
average number of particles per CPU is constant.
<P>
For three dimensional CPU arrays we have to carry out the two
dimensional transport for each CPU layer. Each value is then
transfered cyclically to the next CPU layer.
